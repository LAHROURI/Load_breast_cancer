{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LAHROURI/Load_breast_cancer/blob/main/Load_breast_cancer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yT8cJTGCDbQ"
      },
      "source": [
        "#Dataset load_breast_cancer()\n",
        "\n",
        "La classification des tumeurs du sein à l'aide de différentes méthodes de Machine Learning.\n",
        "L'objectif est de prédire si une tumeur est bénigne ou maligne en utilisant l'ensemble de données `load_breast_cancer()` de scikit-learn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L'imporation des bibliothèques**"
      ],
      "metadata": {
        "id": "wk3_Efhvhq1j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4J7bVFA7zu0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import time\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. chargement les données\n",
        "2. création du DataFrame\n",
        "3. L'ajout de la colonne cible"
      ],
      "metadata": {
        "id": "u0QSUgXZjhQ3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDbQehGiCUY2"
      },
      "outputs": [],
      "source": [
        "data = load_breast_cancer()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "un résumé (`df.info()`) pour connaître les types de données et l'absence de valeurs manquantes."
      ],
      "metadata": {
        "id": "lAgZydfUj4PW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIoIWgq6C2yV"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prétraitement"
      ],
      "metadata": {
        "id": "GArseh55xYPh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSB5wjttGIt9"
      },
      "source": [
        "Remplir les valeurs manquants par la moyenne parce que touts les enregistrements sont des valeurs numériques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZAVnzuWF_Ng"
      },
      "outputs": [],
      "source": [
        "#remplir les valeurs manquants\n",
        "df.fillna(df.mean(), inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Séparation X et y"
      ],
      "metadata": {
        "id": "87RwKczHkEXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('target', axis=1)\n",
        "y = df['target']"
      ],
      "metadata": {
        "id": "Z4jizVBdzHI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encodage de la cible"
      ],
      "metadata": {
        "id": "yFbzm2zUkNE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)"
      ],
      "metadata": {
        "id": "-cuOMiL0yzD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mise à l'échelle des caractéristiques"
      ],
      "metadata": {
        "id": "Ri1qfIb0kdUz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loWfTViOGlov"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Division des données."
      ],
      "metadata": {
        "id": "maYbjtyektkS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwVo-nJQG3fK"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traitement"
      ],
      "metadata": {
        "id": "edjanstd0nY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "vqNtlrVk0q_m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gDdZr_dG87k"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - Random Forest"
      ],
      "metadata": {
        "id": "vB_OKDhVk45H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAucYn31HEGy"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Random Forest Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Prediction - Random Forest"
      ],
      "metadata": {
        "id": "jUnl6KHLk7Tm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opy2Ia41HLOt"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for Random Forest\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report - Random Forest"
      ],
      "metadata": {
        "id": "qr7a2RxGlBfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_reports = {}\n",
        "classification_reports['Random Forest'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "oN_OsI4f3Oa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naïve Bayes**"
      ],
      "metadata": {
        "id": "dOOZdLXr0yHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - Naïve Bayes"
      ],
      "metadata": {
        "id": "rDTAwanKlWJ6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2wmCrCVHcJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Naïve Bayes Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model prediction - Naïve Bayes"
      ],
      "metadata": {
        "id": "oTlEkUZ-lLWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrFTChy4Hma8"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for Naïve Bayes\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report - Random Forest"
      ],
      "metadata": {
        "id": "_9SGF4T1mEvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_reports['Random Forest'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "FNKrsY4ImKcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM Classifier**"
      ],
      "metadata": {
        "id": "xrx30Fta058B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - SVM"
      ],
      "metadata": {
        "id": "FkFAEqNGld_N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLJZZlqeH1Og"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"SVM Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Confusion Matrix Visualization for SVM"
      ],
      "metadata": {
        "id": "pqXsSjfPlmE7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sthMCQhLH5vq"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for SVM\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report - SVM"
      ],
      "metadata": {
        "id": "LQ2s1BnMmTk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classification_reports['SVM'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_MNXk3N9mWoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost Classifier**"
      ],
      "metadata": {
        "id": "4yBHrnFZ1BBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - AdaBoost"
      ],
      "metadata": {
        "id": "IPzfNOHPm1Ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sRjVgSYH-9x"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "clf = AdaBoostClassifier(n_estimators=100)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Confusion Matrix Visualization for AdaBoost"
      ],
      "metadata": {
        "id": "o3lVXcQ0m4H-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDOsmYY8IQqR"
      },
      "outputs": [],
      "source": [
        "cmd = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cmd, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for AdaBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification report"
      ],
      "metadata": {
        "id": "pLVRAklBm_K5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-GTtHYnITDK"
      },
      "outputs": [],
      "source": [
        "classification_reports['AdaBoost'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dession Tree**"
      ],
      "metadata": {
        "id": "kOCQn0xo1H00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - Dession Tree"
      ],
      "metadata": {
        "id": "HuZLDOapnCdU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq2etL63IbaJ"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Decision Tree Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "GwF61ryUnH_h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsrJ9DtpIhEj"
      },
      "outputs": [],
      "source": [
        "cmd = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cmd, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for Decision Tree\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification reports"
      ],
      "metadata": {
        "id": "CnJ3MnRrnKnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7-bcGFVIi7_"
      },
      "outputs": [],
      "source": [
        "classification_reports['Decision Tree'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression logistic**"
      ],
      "metadata": {
        "id": "KE0NBwa_1gIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - regression logistic"
      ],
      "metadata": {
        "id": "8O069S3hnPKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2m9mjpfIojE"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "1bjkiVesnUcT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqKH85UIIs-j"
      },
      "outputs": [],
      "source": [
        "cmd = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cmd, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for Logistic Regression\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report"
      ],
      "metadata": {
        "id": "QOXE_qNPnYf_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4izLb5GIvX2"
      },
      "outputs": [],
      "source": [
        "classification_reports['Logistic Regression'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNN**"
      ],
      "metadata": {
        "id": "RZsRrcVE1ksF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Evaluation - KNN"
      ],
      "metadata": {
        "id": "ZpWkXWcbnbks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3JAlNjfIxwu"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "clf = KNeighborsClassifier(n_neighbors=5)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"KNN Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Visualization"
      ],
      "metadata": {
        "id": "qb0n7W17nhG7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQTSdCefI8KH"
      },
      "outputs": [],
      "source": [
        "cmd = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cmd, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix for KNN\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification report"
      ],
      "metadata": {
        "id": "7wv8o6o0nkih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7t9-RXfI_mX"
      },
      "outputs": [],
      "source": [
        "classification_reports['KNN'] = classification_report(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparaison des Classification Reports**"
      ],
      "metadata": {
        "id": "frpXZHli5C-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Afficher tous les classification reports stockés\n",
        "print(\"=================================================\")\n",
        "print(\"             Comparison of Classification Reports\")\n",
        "print(\"=================================================\")\n",
        "\n",
        "for algo_name, report in classification_reports.items():\n",
        "    print(f\"\\n--- {algo_name} ---\")\n",
        "    print(report)\n",
        "\n",
        "print(\"=================================================\")"
      ],
      "metadata": {
        "id": "uNA2Ph3h2bLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traitement avec GridSearchCV"
      ],
      "metadata": {
        "id": "GHxPsA988Rbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définir les classificateurs avec les paramètres de chaque modèles utilisé dans le traitement"
      ],
      "metadata": {
        "id": "CzdSHnGnn5fI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir les classificateurs et leurs grilles de paramètres respectives\n",
        "classifiers = [\n",
        "    {\n",
        "        'name': 'Random Forest',\n",
        "        'estimator': RandomForestClassifier(random_state=42),\n",
        "        'param_grid': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Naïve Bayes',\n",
        "        'estimator': GaussianNB(),\n",
        "        'param_grid': {\n",
        "            # GaussianNB n'a généralement pas beaucoup de paramètres à régler,\n",
        "            'var_smoothing': [1e-9, 1e-8, 1e-7]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'SVM Classifier',\n",
        "        'estimator': SVC(kernel='linear', probability=True, random_state=42), # probability=True pour roc_auc\n",
        "        'param_grid': {\n",
        "            'C': [0.1, 1, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'AdaBoost Classifier',\n",
        "        'estimator': AdaBoostClassifier(random_state=42),\n",
        "        'param_grid': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'learning_rate': [0.01, 0.1, 1.0]\n",
        "        }\n",
        "    },\n",
        "     {\n",
        "        'name': 'Decision Tree',\n",
        "        'estimator': DecisionTreeClassifier(random_state=42),\n",
        "        'param_grid': {\n",
        "            'max_depth': [None, 10, 20],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        'name': 'Logistic Regression',\n",
        "        'estimator': LogisticRegression(random_state=42, solver='liblinear'), # Utiliser un solver approprié\n",
        "        'param_grid': {\n",
        "            'C': [0.01, 0.1, 1, 10],\n",
        "             'penalty': ['l1', 'l2'] # l1 et l2 sont courants pour liblinear\n",
        "        }\n",
        "    },\n",
        "     {\n",
        "        'name': 'KNN',\n",
        "        'estimator': KNeighborsClassifier(),\n",
        "        'param_grid': {\n",
        "            'n_neighbors': [3, 5, 7, 9],\n",
        "            'weights': ['uniform', 'distance']\n",
        "        }\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "a8nPpogg8TNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Le score AUC** : \"Area Under the Curve\", ou \"Aire sous la courbe\". Il est utilisé pour évaluer les performances d’un modèle de classification binaire, en particulier à partir de la courbe ROC (Receiver Operating Characteristic).\n",
        "\n",
        "**Courbe ROC** : C’est une courbe qui trace le taux de vrais positifs (sensibilité) en fonction du taux de faux positifs (1 - spécificité).\n",
        "\n",
        "Elle montre comment le modèle se comporte à différents seuils de décision.\n",
        "\n",
        "**Score AUC (ROC AUC Score)** :\n",
        "Le score AUC correspond à l’aire sous cette courbe.\n",
        "Il varie entre 0 et 1 :\n",
        "\n",
        "*   1.0 : modèle parfait.\n",
        "*   0.5 : modèle aléatoire (pas mieux qu’un pile ou face).\n",
        "*   < 0.5 : modèle inverse (pire qu’aléatoire).\n",
        "\n",
        "Plus le score est proche de 1, meilleur est le modèle pour séparer les deux classes (positif et négatif).\n",
        "\n",
        "C’est un indicateur robuste, car il prend en compte toutes les valeurs possibles du seuil de décision, pas seulement une valeur fixe."
      ],
      "metadata": {
        "id": "benJDi9xoQ3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionnaires pour stocker les meilleurs résultats\n",
        "best_estimators = {}\n",
        "best_params = {}\n",
        "accuracies = {}\n",
        "classification_reports = {}\n",
        "confusion_matrices = {}\n",
        "roc_auc_scores = {} # Pour stocker les scores AUC"
      ],
      "metadata": {
        "id": "5rEYtay48rXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boucler sur chaque classificateur pour faire la recherche de grille\n",
        "print(\"Starting GridSearchCV for all classifiers...\")\n",
        "for classifier_info in classifiers:\n",
        "    name = classifier_info['name']\n",
        "    estimator = classifier_info['estimator']\n",
        "    param_grid = classifier_info['param_grid']\n",
        "\n",
        "    print(f\"\\n--- Running GridSearchCV for {name} ---\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    grid_search = GridSearchCV(estimator, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "    # Exécuter la recherche de grille sur les données d'entraînement\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"GridSearchCV for {name} finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # Meilleurs paramètres et meilleur score\n",
        "    best_params[name] = grid_search.best_params_\n",
        "    best_estimators[name] = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"Best parameters for {name}: {best_params[name]}\")\n",
        "    print(f\"Best cross-validation score for {name}: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    # Évaluer le meilleur estimateur sur l'ensemble de test\n",
        "    y_pred = best_estimators[name].predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies[name] = accuracy\n",
        "    print(f\"{name} Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Générer et stocker le classification report\n",
        "    classification_reports[name] = classification_report(y_test, y_pred)\n",
        "\n",
        "    # Générer et stocker la matrice de confusion\n",
        "    confusion_matrices[name] = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Calculer et stocker le score ROC AUC si le modèle le permet (nécessite predict_proba ou decision_function)\n",
        "    try:\n",
        "        if hasattr(best_estimators[name], \"predict_proba\"):\n",
        "            y_probs = best_estimators[name].predict_proba(X_test)[:, 1]\n",
        "        elif hasattr(best_estimators[name], \"decision_function\"):\n",
        "             # SVM avec kernel='linear' a decision_function, pas predict_proba par défaut\n",
        "             y_probs = best_estimators[name].decision_function(X_test)\n",
        "        else:\n",
        "            y_probs = None # Modèle ne supporte pas predict_proba/decision_function pour AUC\n",
        "\n",
        "        if y_probs is not None:\n",
        "             roc_auc = roc_auc_score(y_test, y_probs)\n",
        "             roc_auc_scores[name] = roc_auc\n",
        "             print(f\"{name} ROC AUC Score: {roc_auc:.4f}\")\n",
        "        else:\n",
        "             print(f\"{name} does not support ROC AUC calculation.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate ROC AUC for {name}: {e}\")\n",
        "        roc_auc_scores[name] = None # Indiquer que le calcul a échoué\n",
        "\n",
        "\n",
        "print(\"\\nGridSearchCV for all classifiers completed.\")"
      ],
      "metadata": {
        "id": "FR9FKZS285st"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Résultats de la GridSearchCV et Comparaison"
      ],
      "metadata": {
        "id": "aczERoH29pt-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=================================================\")\n",
        "print(\"       Comparison of Classifier Performance (after GridSearchCV)\")\n",
        "print(\"=================================================\")\n",
        "\n",
        "# Afficher les meilleurs paramètres trouvés\n",
        "print(\"\\n--- Best Parameters Found ---\")\n",
        "for name, params in best_params.items():\n",
        "    print(f\"{name}: {params}\")\n",
        "\n",
        "# Afficher les précisions sur l'ensemble de test\n",
        "print(\"\\n--- Test Set Accuracies ---\")\n",
        "for name, acc in accuracies.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n",
        "\n",
        "# Afficher les scores ROC AUC sur l'ensemble de test\n",
        "print(\"\\n--- Test Set ROC AUC Scores ---\")\n",
        "for name, auc in roc_auc_scores.items():\n",
        "    if auc is not None:\n",
        "        print(f\"{name}: {auc:.4f}\")\n",
        "    else:\n",
        "         print(f\"{name}: N/A\") # Not Available si le calcul a échoué\n",
        "\n",
        "# Afficher tous les classification reports stockés\n",
        "print(\"\\n=================================================\")\n",
        "print(\"             Classification Reports (after GridSearchCV)\")\n",
        "print(\"=================================================\")\n",
        "\n",
        "for algo_name, report in classification_reports.items():\n",
        "    print(f\"\\n--- {algo_name} ---\")\n",
        "    print(report)\n",
        "\n",
        "print(\"=================================================\")"
      ],
      "metadata": {
        "id": "mU5ESJPU9Ouo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisation ROC Curves"
      ],
      "metadata": {
        "id": "5rH-hJHF2_vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracer les courbes ROC pour les modèles qui supportent predict_proba/decision_function\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for name, estimator in best_estimators.items():\n",
        "    if name in roc_auc_scores and roc_auc_scores[name] is not None:\n",
        "        try:\n",
        "            if hasattr(estimator, \"predict_proba\"):\n",
        "                 y_probs = estimator.predict_proba(X_test)[:, 1]\n",
        "            elif hasattr(estimator, \"decision_function\"):\n",
        "                 y_probs = estimator.decision_function(X_test)\n",
        "            else:\n",
        "                 continue # Skip if neither is available\n",
        "\n",
        "            fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "            auc_score = roc_auc_score(y_test, y_probs) # Recalculer AUC pour la légende\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.2f})')\n",
        "        except Exception as e:\n",
        "             print(f\"Error plotting ROC curve for {name}: {e}\")\n",
        "             continue\n",
        "\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRHlVjYh3JEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n=================================================\")\n",
        "print(\"             Confusion Matrices (after GridSearchCV)\")\n",
        "print(\"=================================================\")\n",
        "#\n",
        "for algo_name, cm in confusion_matrices.items():\n",
        "     print(f\"\\n--- {algo_name} ---\")\n",
        "     sns.heatmap(cm, annot=True, fmt='d')\n",
        "     plt.title(f\"Confusion Matrix for {algo_name}\")\n",
        "     plt.show()\n",
        "     print(cm) # Afficher la matrice sous forme de tableau\n",
        "\n",
        "print(\"=================================================\")"
      ],
      "metadata": {
        "id": "2yiHTVOiHolz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Les Features Importants dans la dataset"
      ],
      "metadata": {
        "id": "qo122arT94QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_rf_estimator = best_estimators.get('Random Forest')\n",
        "\n",
        "\n",
        "if best_rf_estimator is not None:\n",
        "  importances = best_rf_estimator.feature_importances_\n",
        "  feature_names = data.feature_names\n",
        "\n",
        "  # Create a DataFrame for better visualization\n",
        "  feature_importance_df = pd.DataFrame({\n",
        "      'feature': feature_names,\n",
        "      'importance': importances\n",
        "  })\n",
        "\n",
        "  # Sort by importance in descending order\n",
        "  feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "  print(\"\\n--- Feature Importances from Best Random Forest Model ---\")\n",
        "  print(feature_importance_df)\n",
        "\n",
        "  # Optional: Visualize the feature importances\n",
        "  plt.figure(figsize=(12, 8))\n",
        "  sns.barplot(x='importance', y='feature', data=feature_importance_df)\n",
        "  plt.title('Feature Importances (Random Forest)')\n",
        "  plt.xlabel('Importance')\n",
        "  plt.ylabel('Feature')\n",
        "  plt.show()\n",
        "else:\n",
        "  print(\"\\nRandom Forest estimator not found in best_estimators.\")\n"
      ],
      "metadata": {
        "id": "euUWzXnzA6ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code d'utilisateur"
      ],
      "metadata": {
        "id": "Welg-o7_GaYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ipywidgets import FloatText, VBox, Button, Label\n",
        "from IPython.display import display\n",
        "import numpy as np\n",
        "\n",
        "important_features = ['worst area', 'worst concave points', 'worst perimeter', 'mean concave points']\n",
        "feature_inputs = {}\n",
        "for feature in important_features:\n",
        "    feature_inputs[feature] = FloatText(description=feature + ':')\n",
        "\n",
        "output_label = Label(value=\"\")\n",
        "\n",
        "def predict_cancer_type(b):\n",
        "    # Get user input values\n",
        "    user_input_values = {}\n",
        "    for feature in important_features:\n",
        "        user_input_values[feature] = feature_inputs[feature].value\n",
        "\n",
        "    feature_indices = [list(data.feature_names).index(f) for f in important_features]\n",
        "\n",
        "    # Create a zero-filled array for a single sample (30 features)\n",
        "    single_sample = np.zeros(len(data.feature_names))\n",
        "\n",
        "    # Fill in the values for the important features\n",
        "    for i, feature_name in enumerate(important_features):\n",
        "        original_index = list(data.feature_names).index(feature_name)\n",
        "        single_sample[original_index] = user_input_values[feature_name]\n",
        "\n",
        "\n",
        "    # Scale the single sample using the same scaler fitted on the training data\n",
        "    single_sample_scaled = scaler.transform([single_sample]) # transform expects a 2D array\n",
        "\n",
        "    best_model_name = 'Random Forest'\n",
        "\n",
        "    best_model = best_estimators.get(best_model_name)\n",
        "\n",
        "    if best_model is None:\n",
        "        output_label.value = \"Error: Best model not found.\"\n",
        "        return\n",
        "\n",
        "    prediction = best_model.predict(single_sample_scaled)\n",
        "\n",
        "    # Decode the prediction (0 for benign, 1 for malignant)\n",
        "    predicted_class = le.inverse_transform(prediction)\n",
        "\n",
        "    # Display the result\n",
        "    if predicted_class[0] == 0:\n",
        "        output_label.value = f\"Prediction ({best_model_name}): Benign\"\n",
        "    else:\n",
        "        output_label.value = f\"Prediction ({best_model_name}): Malignant\"\n",
        "\n",
        "\n",
        "# Create the button\n",
        "predict_button = Button(description=\"Predict\")\n",
        "predict_button.on_click(predict_cancer_type)\n",
        "\n",
        "# Arrange the inputs and button\n",
        "input_widgets = [feature_inputs[feature] for feature in important_features]\n",
        "display(VBox(input_widgets + [predict_button, output_label]))\n"
      ],
      "metadata": {
        "id": "3rBshAhpGjWN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyM1dC64FjA5gm8/xYT0WU/B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}